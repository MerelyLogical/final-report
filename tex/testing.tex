\chapter{Testing}

\section{Functional Correctness}
\subsection{Simulation}
During implementation, the modules were simulated with ModelSim to verify their correctness.
Initially we followed the data path of randomiser, driver, monitor, sub-monitor, and scoreboard, each module was attached and simulated.
As the randomiser was a perfect source for setting up random tests for the following module, this process was relatively straightforward once the randomiser was ascertained to work correctly.
Once the system design became stable, the testbench module can be simulated as a whole.
Since RTL simulation is cheap, this is done after any design addition or change thereafter.
As writing hardware can be a lot less intuitive than writing software, the simulations have prevented many errors from going on to the hardware.

\subsection{FPGA Testing}
After each major functional addition, the framework is tested as whole on the development board.
This means that it is integrated with a test DUT, usually an adder, and the entire system is synthesised and programmed on the FPGA.
A basic test script was written early on to for this system level testing.
The FPGA testing serves to confirm the functional correctness shown by the software simulations.

However, the confirmatory nature of the FPGA tests stopped after the number of sub-monitors instantiated by the monitor was made configurable.
The test results no longer agrees with simulation results.
To discover the cause of this discrepancy, another debug method was suggested by my supervisor.

\subsection{Post-fit Simulation}
Previously, the simulation was done with modules files before synthesis.
In order to be closer to hardware, we can synthesise and fit the design with first, then with the \textit{EDA Netlist Writer}, a massive Verilog file can be generated containing the entire design.
This can then be fed into ModelSim for a more accurate simulation.

With this, a signal was found to be off by a clock cycle so we fixed it in the design.
However, this was not sufficient, as the FPGA test results still differ from that of the post-fit simulation.
The next proposed debug method by my supervisor is to use \textit{SignalTap}, which can probe signals inside of the FPGA, allowing us to see the actual waveforms in hardware.
Being a much more time-consuming method, we decided to first take a closer look at the design code before committing to it.
Among other issues, my supervisor was able to identify a clock which I have handled rather carelessly.
After spending some time tidying up this clock, the system again behaved correctly as predicted by the simulation.

\section{Maximum Frequency}
As a important benchmark of the framework, the maximum frequency is closely monitored during the implementation process.
This is done with \textit{TimeQuest Timing Analyser}, which is a tool that can provide an estimate on the maximum frequency the testbench and run safely on.
If this value is lower than what is required, the tool can also provides a list of the slowest offending signal paths for us to optimise on.

During FPGA tests, the physical maximum frequency the board can achieve is measured with a script that will run the same test repeatedly with increasing frequencies.
These are useful as the software estimations are usually more conservative than what is possible on hardware.

\section{Out-of-the-box Testing}
\subsection{Introduction}
In an out-of-the-box(OOTB) test, the product is delivered to test users as a packaged box.
Its unpacking process, in which the users sets up and uses the product, is then observed to study the intuitiveness of the design.
This testing method is used in this project as one of the key determinant for the success of this project is how convenient it is for users to configure and make use of the framework.

For this project, we have made up a scenario where the users have designed an adder with a suspected error that they wish to ascertain.
The user guide [Appendix \ref{appendix:guide}], the testbench, and a flawed adder design was provided to the test volunteers.
An extra piece of logic was added to an adder design, so that if both inputs are odd, the adder will produce an incorrect output.
The test volunteers are made aware of the fact that there might be an error relating to the parity (evenness) of the inputs.

Unfortunately, due to time constraints and the limited number of people with Quartus knowledge that I have access to, this test was only performed by me first and one volunteer after.
Nevertheless, this process still proved to be helpful with many design flaws being identified, and a list of possible improvements being drawn out.
The possible improvements will be discussed in the \textit{Further Work} chapter, while the identified flaws will be evaluated within this chapter.

\subsection{Hardware Configuration}
For the hardware portion, the test user were given adders of width 16 which is expected to work up to 3 times as fast as the reference.
He was able to correctly modify the default values for \texttt{WIDTH} where is 32, and for \texttt{NUM\_SUB\_MON}, where the default is 2.
However, there seemed to be an issue with Qsys where the widths of ports are not always updated when the module parameters are modified.
As he had not much familiarity with Qsys, and was not expected to have any knowledge of the inner workings of the \texttt{test\_wrapper} module, the test had to be interrupted.
To fix this, we deleted and re-added both the \texttt{test\_wrapper} and the \texttt{dut\_adder} module, modifying their their default values before their were placed into the \textit{System Contents} window.
A note was also added in the user guide.

After dealing with the minor obstacle, he was able to generate the HDL from Qsys, and compile the full design with Quartus without issues.

\subsection{Using the Test Software}
The uploading and the programming procedure was smooth, the volunteer followed the guide and successfully primed the development board for testing.

He then had to read the command list of the testing software and devise a plan to pinpoint the relationship between the parity of the inputs and the correctness of the design output.
At this point, he suggested that while some of the commands were explained succinctly, the commands on manipulating the inputs were not explained in sufficient detail and left him somewhat confused.
As such, I explained the details and improved the user guide accordingly.

While explaining, the volunteer quickly realised that he can use the \texttt{bitset} and \texttt{bitclr} commands to fix the evenness of the inputs.
However, in doing so he typed in a series of instructions that the test software never had to deal with during initial testing.
This revealed a bug in the software where the frequency of the PLL output is not well defined if the software issued a reset command after setting the frequency.

After a second interruption to fix this bug, the test was able to continue.
With some experimenting in the REPL interface, the user was able to correctly conclude that the design gives incorrect values only when both inputs are odd.
Test automation was also tested, in which the volunteer had no trouble getting the software to run with his command list file.

\subsection{Testing Results}
Despite the two intermissions, the OOTB was complete within 2 hours and reportedly a smooth experience for the test volunteer.
This showed that the framework is reasonably intuitive and user-friendly.
However, the test also highlighted a few major places where the project can do better in.

Over the entire duration of the OOTB testing process, a significant amount of time was wasted fiddling with Quartus and Qsys GUIs.
It was also the step most where the framework is the most prone to user mistakes.
For example, a misclick in the Qsys \textit{System Contents} window may only cause an error during the synthesis of the entire project, and for someone not familiar with Qsys, this error may take a very long time to be identified and fixed.
As such, having a way to automate the hardware configuration process would be a great improvement to the usability of the product.
Furthermore, a unified configuration system would be even better, as the user will not have to figure out what customisation options have to be done in the GUIs before compile, and what can still be manipulated during in the testing software.
A method of accomplishing both will be proposed in the \textit{Further Work} chapter.

Another key improvement arising from the OOTB testing was on the readability of the user guide.
It is difficult to produce a guide that will be suitable to readers in all skill levels.
Therefore, while this one test has made it slightly better, feedback from more users and especially users with different levels of experience and knowledge is definitely still needed.
